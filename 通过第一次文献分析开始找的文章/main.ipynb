{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_path = '/Users/dong/Desktop/Article_1_prepare/第一次文献keyword分析/wos_record.txt'\n",
    "scopus_path = '/Users/dong/Desktop/Article_1_prepare/第一次文献keyword分析/scopus_record.csv'\n",
    "pubmed_path = '/Users/dong/Desktop/Article_1_prepare/第一次文献keyword分析/pubmed_record.txt'\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "## Identifying, union, merge articles from PubMed, Scopus, and WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scopus(filepath):\n",
    "    \"\"\"Parse Scopus CSV data and adjust the column names.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # 重命名列以符合期望的数据结构\n",
    "    df.rename(columns={\n",
    "        'Source title': 'Journal',  # 确保将'Source title'重命名为'Journal'\n",
    "        'Title': 'Title',\n",
    "        'Authors': 'Authors',\n",
    "        'Year': 'Year',\n",
    "        'DOI': 'DOI'\n",
    "    }, inplace=True)\n",
    "    df['Source'] = 'Scopus'  # 添加数据源列\n",
    "    return df\n",
    "\n",
    "def parse_pubmed(filepath):\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        article = {}\n",
    "        for line in file:\n",
    "            if line.startswith('PMID-'):\n",
    "                if article:\n",
    "                    records.append(article)\n",
    "                article = {'Source': 'PubMed'}\n",
    "            elif line.startswith('TI  - '):\n",
    "                article['Title'] = line.replace('TI  - ', '').strip()\n",
    "            elif line.startswith('AU  - '):\n",
    "                article['Authors'] = line.replace('AU  - ', '').strip()\n",
    "            elif line.startswith('JT  - '):\n",
    "                article['Journal'] = line.replace('JT  - ', '').strip()\n",
    "            elif line.startswith('DP  - '):\n",
    "                article['Year'] = line.replace('DP  - ', '').strip().split()[0]\n",
    "            elif line.startswith('LID - ') and '[doi]' in line:\n",
    "                article['DOI'] = line.replace('LID - ', '').strip().split()[0].replace('[doi]', '')\n",
    "        if article:\n",
    "            records.append(article)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def parse_wos(filepath):\n",
    "    \"\"\"Parse Web of Science data ensuring complete title capture and handling multiline entries for Title specifically.\"\"\"\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        article = {}\n",
    "        current_key = None  # Current field being parsed\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('PT '):  # Start of a new record\n",
    "                if article:\n",
    "                    records.append(article)\n",
    "                article = {'Source': 'WoS'}  # Initialize a new record\n",
    "                current_key = None\n",
    "            if line.startswith('TI '):  # Title may continue on multiple lines\n",
    "                current_key = 'Title'\n",
    "                article['Title'] = line[3:].strip()  # Initialize title\n",
    "            elif line.startswith('AU '):  # Authors\n",
    "                current_key = 'Authors'\n",
    "                article['Authors'] = line[3:].strip()\n",
    "            elif line.startswith('SO '):  # Journal name\n",
    "                current_key = 'Journal'\n",
    "                article['Journal'] = line[3:].strip()\n",
    "            elif line.startswith('PY '):  # Publication year\n",
    "                current_key = 'Year'\n",
    "                article['Year'] = line[3:].strip()\n",
    "            elif line.startswith('DI '):  # DOI\n",
    "                current_key = 'DOI'\n",
    "                article['DOI'] = line[3:].strip()\n",
    "            elif current_key == 'Title' and not line.startswith('AU ') and not line.startswith('SO ') and not line.startswith('PY ') and not line.startswith('DI '):\n",
    "                # Continue appending to the title if no new field tag is detected\n",
    "                article['Title'] += ' ' + line\n",
    "\n",
    "        if article:  # Append the last processed record\n",
    "            records.append(article)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def standardize_df_columns(df, standard_cols):\n",
    "    # Add missing columns with default values\n",
    "    missing_cols = set(standard_cols) - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df[col] = None  # Assign None or an appropriate default value\n",
    "    # Reorder and select the standard columns\n",
    "    return df[standard_cols]\n",
    "\n",
    "# Parse data\n",
    "pubmed_data = parse_pubmed(pubmed_path)\n",
    "scopus_data = parse_scopus(scopus_path)\n",
    "wos_data = parse_wos(wos_path)\n",
    "\n",
    "# Ensure consistent column names\n",
    "standard_cols = ['Title', 'Authors', 'Journal', 'Year', 'DOI', 'Source']\n",
    "pubmed_data = standardize_df_columns(pubmed_data, standard_cols)\n",
    "scopus_data = standardize_df_columns(scopus_data, standard_cols)\n",
    "wos_data = standardize_df_columns(wos_data, standard_cols)\n",
    "\n",
    "# Concatenate data\n",
    "all_data = pd.concat([pubmed_data, scopus_data, wos_data], ignore_index=True)\n",
    "\n",
    "# Checking and merging publications count\n",
    "journal_counts = all_data['Journal'].value_counts().reset_index()\n",
    "journal_counts.columns = ['Journal', 'Publications']\n",
    "all_data = all_data.merge(journal_counts, on='Journal', how='left')\n",
    "\n",
    "# Sort by publication counts and print the data\n",
    "all_data.sort_values(by=['Publications', 'Journal'], ascending=[False,True], inplace=True)\n",
    "\n",
    "# Save the final data\n",
    "all_data.to_csv('sorted_publications.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "## Delete articles relate with \"marine heatwave\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pd = pd.DataFrame(pd.read_csv('sorted_publications.csv'))\n",
    "sorted_pd['Title'] = sorted_pd['Title'].str.lower()\n",
    "sorted_pd = sorted_pd[sorted_pd['Title'].str.contains('marine')==False]\n",
    "sorted_pd.to_csv('second_process.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "## Delete duplicates completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_pd = pd.DataFrame(pd.read_csv('second_process.csv'))\n",
    "temp_pd.dropna(subset=['Title'], inplace=True)\n",
    "temp_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('second_process.csv')\n",
    "\n",
    "# 找到所有重复的标题\n",
    "duplicates = df[df.duplicated('Title', keep=False)]\n",
    "\n",
    "# 对重复的标题按来源进行聚合\n",
    "title_sources = duplicates.groupby('Title')['Source'].agg(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analysis of heatwaves based on the universal t...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropogenic climate change exacerbates the r...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>behaviour broadens thermal safety margins on a...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>climate change over indonesia and its impact o...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>future changes in high and low flows under the...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>future exposure of rainfall and temperature ex...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heatwaves in southeast asia and their changes ...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how do disparate urbanization and climate chan...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>impacts of and adaptation to climate change on...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>interaction between heat wave and urban heat i...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mitigating intensity of urban heat island by b...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>projections and patterns of heat-related morta...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spatial patterns of health vulnerability to he...</td>\n",
       "      <td>[WoS, Scopus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title         Source\n",
       "0   analysis of heatwaves based on the universal t...  [WoS, Scopus]\n",
       "1   anthropogenic climate change exacerbates the r...  [WoS, Scopus]\n",
       "2   behaviour broadens thermal safety margins on a...  [WoS, Scopus]\n",
       "3   climate change over indonesia and its impact o...  [WoS, Scopus]\n",
       "4   future changes in high and low flows under the...  [WoS, Scopus]\n",
       "5   future exposure of rainfall and temperature ex...  [WoS, Scopus]\n",
       "6   heatwaves in southeast asia and their changes ...  [WoS, Scopus]\n",
       "7   how do disparate urbanization and climate chan...  [WoS, Scopus]\n",
       "8   impacts of and adaptation to climate change on...  [WoS, Scopus]\n",
       "9   interaction between heat wave and urban heat i...  [WoS, Scopus]\n",
       "10  mitigating intensity of urban heat island by b...  [WoS, Scopus]\n",
       "11  projections and patterns of heat-related morta...  [WoS, Scopus]\n",
       "12  spatial patterns of health vulnerability to he...  [WoS, Scopus]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pd.drop_duplicates(subset=['Title'],inplace = True)\n",
    "temp_pd.to_csv('Third_process.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "## Fetch title related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pd3 = pd.DataFrame(pd.read_csv('Third_process.csv'))\n",
    "selected_articles = temp_pd3[temp_pd3['Yes No'] == 1]\n",
    "selected_articles = selected_articles.iloc[:,1:]\n",
    "selected_articles.to_csv('forth_process.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(398, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detected_drop_articles = temp_pd3[temp_pd3['Yes No'] == 0]\n",
    "detected_drop_articles.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
